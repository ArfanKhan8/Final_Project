{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389c9440-1da9-460c-b606-12b2a0c7b2fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f19df5-2250-4f81-83e8-c0388d19e94c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7e32c5-cd3a-4287-ab7d-9e39079f8c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('WELFake_Dataset.csv')\n",
    "# df.head()\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66993150-2c2f-47f2-b34e-09a1fbbd5497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 20:23:58 WARN Utils: Your hostname, Kamals-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.149 instead (on interface en0)\n",
      "23/12/10 20:23:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 20:23:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a Spark Session\n",
    "spark = SparkSession.builder.appName(\"FakeNewsDetection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84e1a1c-eb77-43f9-8f7c-f315bff4ea24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58634"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into Spark DataFrame\n",
    "dataset = 'WELFake_Dataset.csv'\n",
    "schema = StructType([\n",
    "    StructField('Index',IntegerType(),True),\n",
    "    StructField('title',StringType(),True),\n",
    "    StructField('text',StringType(),True),\n",
    "    StructField('label',IntegerType(),True)\n",
    "])\n",
    "df = spark.read.format('csv').option('header','true').option(\"mode\", \"DROPMALFORMED\").schema(schema).load(dataset)\n",
    "df = df.na.drop(subset=[\"text\"])\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0566645-523e-4d22-afef-2a23c37072ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(inputCol='text',outputCol='tokenized_text')\n",
    "# df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2c9aac-a2f5-4f63-9044-a68d9172f60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting words to features using HashingTF\n",
    "hashingTF = HashingTF(inputCol='tokenized_text',outputCol='features')\n",
    "# df = hashingTF.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bdeade1-a24e-4da6-b951-8bfbdda3ba34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "train, test = df.randomSplit([0.8,0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4503e456-e213-4434-b02b-9debf487491f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7463e4b9-37ca-4cad-9e36-c82aa349ea80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipleine with the stages\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bee7512-521b-4ed1-8b41-08eef8a12122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 20:24:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , title, text, label\n",
      " Schema: Index, title, text, label\n",
      "Expected: Index but found: \n",
      "CSV file: file:///Users/kamalnath/Documents/BDAT/1008_Data_Collection_Curation/Final%20Project/WELFake_Dataset.csv\n",
      "23/12/10 20:24:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/10 20:24:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , title, text, label\n",
      " Schema: Index, title, text, label\n",
      "Expected: Index but found: \n",
      "CSV file: file:///Users/kamalnath/Documents/BDAT/1008_Data_Collection_Curation/Final%20Project/WELFake_Dataset.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b2edeb-d666-44b7-86b6-ee69c34ad5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70da9196-ab60-4ac7-93e9-aa69b1c00bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 20:24:43 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/12/10 20:24:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , title, text, label\n",
      " Schema: Index, title, text, label\n",
      "Expected: Index but found: \n",
      "CSV file: file:///Users/kamalnath/Documents/BDAT/1008_Data_Collection_Curation/Final%20Project/WELFake_Dataset.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label',rawPredictionCol='rawPrediction',metricName='areaUnderROC')\n",
    "area_under_roc = evaluator.evaluate(predictions)\n",
    "print('Area under ROC: {:.2f}'.format(area_under_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f021493e-8099-4554-be09-9fff56c617b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 20:24:51 WARN TaskSetManager: Stage 220 contains a task of very large size (2097 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model.save('fake_news_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "78f4c241-a599-4577-8e39-d0c039e63c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a1103-1dab-4756-8d17-0de753115c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
